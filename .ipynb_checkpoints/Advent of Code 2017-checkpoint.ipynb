{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import requests\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Functions\n",
    "For two different problems I thought I would need a BFS / DFS but didn't end up needing them.  I did however, waste a lot of time debugging them since I'm coding them from memory and they are easy to introduce subtle bugs into (at least for me). Since I'm fairly sure I'll need some search at some point, I'm just going to leave them here for future use (to be expanded as necessary, e.g. I'm not going to bother with `A*` until I need to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bfs(start,goal,successors = hex_neighbors):\n",
    "    frontier = collections.deque([[start]])\n",
    "    seen = set(tuple(start))\n",
    "    while frontier:\n",
    "        path = frontier.popleft()\n",
    "        node = path[-1]\n",
    "        if tuple(node) not in seen:\n",
    "            seen.add(tuple(node))\n",
    "            for s in successors(node):\n",
    "                if s == goal:\n",
    "                    return path,len(path)\n",
    "                frontier.append(path + [s])\n",
    "    return seen\n",
    "\n",
    "def dfs(start,goal,successors = lambda x: x):\n",
    "    frontier = [[start]]\n",
    "    seen = set(tuple(start))\n",
    "    while frontier:\n",
    "        path = frontier.pop()\n",
    "        node = path[-1]\n",
    "        if tuple(node) not in seen:\n",
    "            seen.add(tuple(node))\n",
    "            for s in successors(node):\n",
    "                if s == goal:\n",
    "                    return path,len(path)\n",
    "                frontier.append(path + [s])\n",
    "    return seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day One\n",
    "This should be pretty simple... for part 1 all you need to do is compare adjacent items up until the length of the list minus 1 (minus 1 because at the end, or second to last element, we compare to  i + 1).  Since I'm going for time, rather than spend the extra 30 seconds writing the code to handle when the index goes beyond the list, I just eyeballed it and saw the last element was the same as the first, and added it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('day1.txt').read().strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1069"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([int(x) for x,y in (zip(data, data[1:])) if x == y]) + 3 #cheated :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = len(data) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(data)):\n",
    "    ix = min(i + size,i + size - len(data))\n",
    "    if data[i] == data[ix]:\n",
    "        res.append(int(data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1268"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Two\n",
    "Our input for day two is a series of rows containing tab delimited numbers, meant to mimic a spreadsheet layout.  This is straight-forward to parse by simply splitting on newlines to get the rows, and then doing a regex on each row to get numbers out, rather than dealing with the tabs. Once this is done, you still have to convert each text input to an integer in order to perform the arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = open('aoc2.txt').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = data2.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part one\n",
    "Part one asks us to find the difference of the minimum and maximum of each row, and then sum all of those values to calculate the \"checksum\" which will be our answer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numrows = [list(map(int,re.findall('\\d+',row))) for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37923"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(max(row) - min(row) for row in numrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part two\n",
    "Part two requires us to find the only two evenly divisible items in each row, calculate the value of this division, and then sum the results.  Itertools to the rescue.... I'll just get all combinations of size 2 for each row and test divisibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(max(x,y) / min(x,y) for row in numrows \n",
    "                        for x,y in itertools.combinations(row,2) \n",
    "                        if max(x,y) % min(x,y) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divis = []\n",
    "for row in numrows:\n",
    "    for x,y in itertools.combinations(row,2):\n",
    "        smallest,largest  = min(x,y), max(x,y)\n",
    "        if largest % smallest == 0:\n",
    "            divis.append(largest / smallest)\n",
    "sum(divis)\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'b'), ('a', 'c'), ('b', 'c')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"abc\"\n",
    "res = set()\n",
    "for i in range(len(test)):\n",
    "    for j in range(i+1,len(test)):\n",
    "        res.add((test[i],test[j]))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 \n",
    "The minute I read this puzzle I may have mumbled a few choice words to myself.  I've seen something similar before on project euler and punted on it, because I'm not particularly great at the more \"mathy\" or number theory-esque problem types.  But, since this is Advent of Code, and I'm fully committed to at least doing my very best to solve all 25 days, I'm gonna have to buckle down and try to figure it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "This is my WIP for part one, I havent even gotten to part two.  Day four has come and gone and was fortunately way easiser. I'm still mulling this over and trying to find a solution without looking at others because I feel I'm getting close since I've found the recurrence relation, I just need to figure out how to use that to calculate the distance to the target.\n",
    "\n",
    "<strong>UPDATE</strong>: After basically an entire day of kicking around ideas and ironing out some bugs, I got a working solution for part 1, a bit before day 5 was released.  The code is most likely overly complicated which tends to be the case when I don't have a clear idea how to solve a problem and I'm just iteratively building on ideas until I get to a solution.  Below the solutions to part one and two are some properties I found which I had originally hoped to use to solve part one (a recurrence relation) but in the end couldn't figure out how to get a solution using this approach.\n",
    "\n",
    "I was a bit nervous on what part two was going to be like since most people seemed to think that was much more difficult than the first part, but it turned out to be fairly straight-foward to extend my part one code to solve part two.  I guess I got lucky in the way I implemented part one - my guess is alot of people found a nice analytical solution to part one and then had to scrap and start from nothing for part two.  My lack of math skills saved me from the same folly since I basically had constructed the grid already in part one, which made part two much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "loc = [0,0]\n",
    "target = 265149\n",
    "MOVES = [[0,1],[-1,0],[0,-2],[2,0],[0,2]]\n",
    "INCREMENTS = [[0,0],[-2,0],[0,-2],[2,0],[0,2]]\n",
    "moves_so_far = 1\n",
    "\n",
    "def vector_process(v1,v2,func=lambda x,y: x+y):\n",
    "    return [func(a,b) for a,b in zip(v1,v2)]\n",
    "\n",
    "def move(loc,move,moves_so_far):\n",
    "    new_loc = vector_process(loc,move)\n",
    "    moves_so_far += sum([abs(num) for num in move])\n",
    "    return new_loc,moves_so_far\n",
    "\n",
    "def manhattan_distance(c1,c2):\n",
    "    return sum(vector_process(c1,c2,func=lambda x,y: abs(x-y)))\n",
    "\n",
    "def cumulative_move_steps(move):\n",
    "    x,y = move \n",
    "    moves = []\n",
    "    if x < 0:\n",
    "        for i in range(-1,x-1,-1):\n",
    "            moves.append([-1,y])\n",
    "    elif x > 0:\n",
    "        for i in range(1,x+1):\n",
    "            moves.append([1,y])\n",
    "    elif y < 0:\n",
    "        for i in range(-1,y-1,-1):\n",
    "            moves.append([x,-1])\n",
    "    elif y > 0:\n",
    "        for i in range(1,y+1):\n",
    "            moves.append([x,1])\n",
    "    return moves\n",
    "\n",
    "def part1():\n",
    "    global MOVES,loc,moves_so_far\n",
    "    while True:\n",
    "        for m in MOVES:\n",
    "            for c in cumulative_move_steps(m):\n",
    "                loc,moves_so_far = move(loc,c,moves_so_far)\n",
    "                if moves_so_far >= target:\n",
    "                    return manhattan_distance(loc,[0,0])\n",
    "        MOVES =  [vector_process(m[0],m[1]) \n",
    "                  for m in zip(MOVES,INCREMENTS)]\n",
    "\n",
    "\n",
    "def make_grid(row,col):\n",
    "    return [[0 for i in range(col)] for j in range(row)]\n",
    "print(part1())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266330\n"
     ]
    }
   ],
   "source": [
    "def part2(grid):\n",
    "    MOVES = [[0,1],[-1,0],[0,-2],[2,0],[0,2]]\n",
    "    cur_number = 1\n",
    "    grid_size = len(grid)\n",
    "    center_x,center_y = loc = [grid_size // 2] * 2\n",
    "    grid[center_x][center_y] = 1\n",
    "    def move(loc,move,cur_number):\n",
    "        new_loc = vector_process(loc,move)\n",
    "        cur_number = sum(grid[x][y] for x,y in neighbors(new_loc))\n",
    "        return new_loc,cur_number\n",
    "    def neighbors(loc):\n",
    "        return [vector_process(loc,(i,j)) for i in range(-1,2)\n",
    "                                          for j in range(-1,2) \n",
    "                                          if (0 <= i + loc[0] < grid_size and 0 <= j + loc[1] < grid_size)\n",
    "                                          and (i,j) != (0,0)]\n",
    "    while True:\n",
    "        for m in MOVES:\n",
    "            for c in cumulative_move_steps(m):\n",
    "                loc,cur_number = move(loc,c,cur_number)\n",
    "                grid[loc[0]][loc[1]] = cur_number\n",
    "                if cur_number > target:\n",
    "                # return manhattan_distance(loc,[center_x,center_y]) RTFM\n",
    "                    return cur_number\n",
    "        MOVES =  [vector_process(m[0],m[1]) \n",
    "                  for m in zip(MOVES,INCREMENTS)]\n",
    "\n",
    "\n",
    "print(part2(make_grid(515,515)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following were my original attempts at solving the puzzle before I threw in the towel and slept on it.  The next day I still could'nt figure out how to use these to solve part 1 so I just caved and chose to build the grid as seen above.  I still got some satisfaction out of this though, because I really like recursion and always feel recursive solutions are elegant and beautiful :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diags = [3,9,7,5]\n",
    "lrud =  [2,4,6,8]  \n",
    "\n",
    "def gen_spiral(n,target = target):\n",
    "    n = n\n",
    "    d = n - 1\n",
    "    steps = 1\n",
    "    nums = [n]\n",
    "    while n < target:\n",
    "        d += 8\n",
    "        n += d\n",
    "        steps += 1\n",
    "        nums.append(n)\n",
    "    return nums\n",
    "    \n",
    "for d in diags:\n",
    "    spirals = gen_spiral(d)\n",
    "    if target in spirals:\n",
    "        print(spirals.index(target) * 2)\n",
    "        break\n",
    "for x in lrud:\n",
    "    spirals = gen_spiral(x)\n",
    "    if target in spirals:\n",
    "        print(spirals.index(target))\n",
    "        break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 19, 40, 69, 106, 151, 204, 265, 334, 411]\n"
     ]
    }
   ],
   "source": [
    "print(gen_spiral(6)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 47, 78, 117, 164, 219, 282, 353, 432, 519]\n"
     ]
    }
   ],
   "source": [
    "def gen_spiral(n,prevn):\n",
    "    yield n\n",
    "    d = n - prevn + 8\n",
    "    n += d\n",
    "    if n < target: yield from gen_spiral(n,n-d)\n",
    "\n",
    "print(list(gen_spiral(24,9))[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 9, 25, 49, 81, 121, 169, 225, 289, 361]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spiral(n):\n",
    "    return 1 if n <= 0 else 2 * spiral(n-1) - spiral(n-2) + 8\n",
    "\n",
    "list(map(spiral,range(0,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4\n",
    "Here the input is a colleciton of strings delimited by newlines.  Within each string, or \"phrase\", we need to see if there are any duplicates and filter those out, and then count the remaining \"phrases\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('day4.txt').read().strip()\n",
    "words = data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n"
     ]
    }
   ],
   "source": [
    "valid = [phrase for phrase in words if len(set(phrase.split())) == len(phrase.split())]\n",
    "print(len(valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two\n",
    "To get anagrams of a word I just found all permutations of the word in question, and then checked the rest of the list for each permutation.  This can be wasteful since you end up calculating unnecessary permutations - for example, if your list is \n",
    "\n",
    "['Christian','Christina']\n",
    "\n",
    "Only the last two letters have changed, so you should not consider any permutations where any of the first seven letters differ.  You can do this by creating a list of valid prefixes of your candidates, and making sure to not progress along the permutaitons unless your current permutation is seen in that candidate prefix list.  \n",
    "\n",
    "Because the amount of data I'm dealing with is small, and I am trying to move quickly, I'm not going to prematurely optimize and just brute force it by calculating all permutations, regardless of whether or not they are potentially invalid early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def anagrams(word):\n",
    "    return {''.join(a) for a in (itertools.permutations(word))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def anagram_found(text):\n",
    "    words = text.split()\n",
    "    for i in range(len(words) - 1):\n",
    "        for a in anagrams(words[i]):\n",
    "            if a in words[i+1:]:\n",
    "                return True\n",
    "    return False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(not(anagram_found('abcde fghij')))\n",
    "assert(anagram_found('abcde xyz ecdab'))\n",
    "assert(not(anagram_found('a ab abc abd abf abj')))\n",
    "assert(not(anagram_found('iiii oiii ooii oooi oooo')))\n",
    "assert(anagram_found('oiii ioii iioi iiio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases = [phrase for phrase in words if not(anagram_found(phrase))]\n",
    "len(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ojufqke gpd olzirc jfao cjfh rcivvw pqqpudp',\n",
       " 'wchrl pzibt nvcae wceb',\n",
       " 'rdwytj kxuyet bqnzlv nyntjan dyrpsn zhi kbxlj ivo',\n",
       " 'qwx ubca dxudny oxagv wqrv lhzsl qmsgv dxs awbquc akelgma',\n",
       " 'rrdlfpk ohoszz qiznasf awchv qnvse']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5\n",
    "Day 5 reminds me of some of the problems from last year where you had to build a mini interpreter for a limited set of machine instructions, but on a smaller scale.  The instructions turned out to be fast enough without trying to find a patterm or optimizing the sequence by some form of pre-processing, but I wouldn't be surprised if, like last year, later puzzles build on the sequence and make it prohitively slow without some optmizations.  \n",
    "\n",
    "As presented, it's fairly easy to solve using some globals to keep state, and just bailing once the position is outside of the list.  I do wonder how I'll approach this in a more functional style, since I'm trying to do all of these problems in Clojure once I've finished them in Python first.  Typically whenever I think `while` in an imperative language, I think `reduce` in a functional language, so I'll probably start there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('day5.txt').read().strip().split('\\n')\n",
    "nums = [int(x) for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381680\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "cnt = 0\n",
    "while pos in range(len(nums)):\n",
    "    offset = nums[pos]\n",
    "    nums[pos] += 1\n",
    "    pos += offset\n",
    "    cnt += 1\n",
    "print(cnt)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29717847\n"
     ]
    }
   ],
   "source": [
    "nums = [int(x) for x in data] #rebuild nums since we mutated it in step 1\n",
    "\n",
    "pos = 0\n",
    "cnt = 0\n",
    "while pos in range(len(nums)):\n",
    "    offset = nums[pos]\n",
    "    if offset >= 3:\n",
    "        nums[pos] -=1\n",
    "    else:\n",
    "        nums[pos] +=1\n",
    "    pos += offset\n",
    "    cnt += 1\n",
    "print(cnt)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6\n",
    "This puzzle involves taking a sequence of numbers, or \"memory banks\", each of which has \"n\" blocks.  We are asked to find how many cycles it takes until any sequence is seen a second time, where a cycle is defined as follows:\n",
    "- find the maximum of the sequence, ties are by the first seen (lowest index)\n",
    "- reduce the blocks at this location to zero, and spread that amount over all of the subsequent banks (list elements), wrapping around once the end is hit.\n",
    "- return the number of cycles that have occured once a repeat is hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = [10,3,15,10,5,15,5,15,9,2,5,8,5,2,3,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "I started down the path of a recursive solution, because that seemed the most natural to me at the time.  Just make the updates to the list, add the result to the `seen` set, and recur with an incremented count.  Except I forgot about the 1,000 depth recursion limit in Python.  I guess this is what happens when you spend too much time in Clojure lately.  I burned a good 20 minutes trying to manually increase the recursion limit using `sys.setrecursionlimit(n)` and kept getting an infuriating error `TypeError: 'int' object is not callable`.  I eventually gave up since this isn't the documented behavior and I couldn't figure out why it was failing.  In any event, I probably should not treat an iterative problem with a recursive solution in a language without tail call optmization, so I just converted the solution to a nested `while` loop instead.\n",
    "\n",
    "Here is my recursive version, which worked on the small test input and then exploded on the actual input when the limit surpassed 1,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cycle(nums,res=set(),cnt=1):\n",
    "    highest = max(nums)\n",
    "    loc = nums.index(highest)\n",
    "    nums[loc] = 0\n",
    "    while highest > 0:\n",
    "        loc += 1\n",
    "        if loc > len(nums) - 1:\n",
    "            loc =  loc % (len(nums))\n",
    "        nums[loc] += 1\n",
    "        highest -= 1\n",
    "    if tuple(nums) in res:\n",
    "        return nums,cnt\n",
    "    res.add(tuple(nums))\n",
    "    return cycle(nums,res,cnt+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here is the version I had to replace it with after giving up on changing the recursion limit (which is obviously bad form, but I had already invested time in a working solution and wanted to not burn more time by re-writing (which didn't really take that much effort in the end anyhow).  Something about this solution rubs me the wrong way, it doesn't feel that elegant but it gets the job done.  I think it's the nested while loops that are giving me that feeling.  Anyhow - when I'm trying to solve these fast I focus on just getting something working first, making it pretty comes later.  Maybe I'll clean all these up and show the first version and the second polished version when I have more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 15, 14, 13, 12, 10, 10, 9, 8, 7, 6, 4, 3, 5] 14029\n"
     ]
    }
   ],
   "source": [
    "nums = [10,3,15,10,5,15,5,15,9,2,5,8,5,2,3,6] #rebind since mutated above\n",
    "seen = set()\n",
    "cnt = 1\n",
    "while True:\n",
    "    highest = max(nums)\n",
    "    loc = nums.index(highest)\n",
    "    nums[loc] = 0\n",
    "    while highest > 0:\n",
    "        loc += 1\n",
    "        if loc > len(nums) - 1:\n",
    "            loc =  loc % (len(nums))\n",
    "        nums[loc] += 1\n",
    "        highest -= 1\n",
    "    if tuple(nums) in seen:\n",
    "        print(nums,cnt)\n",
    "        break\n",
    "    seen.add(tuple(nums))\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two\n",
    "Here the problem is modified such that instead of finding the cycles, we find the distance between cycles.  E.G. if the first occurence was on our third cycle, and the repeat of that occurence was at the 10th cycle, the distance is seven.  The only modificaton I need to make is swapping out my `seen` set for a dictionary, with the key as the actual sequence (tuple so it's hashable), and the value as the cycle it occured in.  On repeat I just find the current count minus the previous count, which i look up using the current sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 15, 14, 13, 12, 10, 10, 9, 8, 7, 6, 4, 3, 5] 2765\n"
     ]
    }
   ],
   "source": [
    "nums = [10,3,15,10,5,15,5,15,9,2,5,8,5,2,3,6] #rebind since mutated above\n",
    "seen = dict()\n",
    "cnt = 1\n",
    "while True:\n",
    "    highest = max(nums)\n",
    "    loc = nums.index(highest)\n",
    "    nums[loc] = 0\n",
    "    while highest > 0:\n",
    "        loc += 1\n",
    "        if loc > len(nums) - 1:\n",
    "            loc =  loc % (len(nums))\n",
    "        nums[loc] += 1\n",
    "        highest -= 1\n",
    "    if tuple(nums) in seen:\n",
    "        print(nums,cnt - seen[tuple(nums)])\n",
    "        break\n",
    "    seen[tuple(nums)] = cnt\n",
    "    cnt += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Seven\n",
    "This puzzle is about trees and recursion - two things that typically go well together.  We are given a tree in the form of an adjacency list, but we don't know where the root of the tree is - our task is to find it.  Typically when traversing a tree, you start at the root and either recursively evaluate children (for depth first search), or maintain a queue and evaluate children left to right (for breadth first search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "\n",
    "I found this problem a little tricky - typically when working with tree or graph structures I know where I'm starting, here I had to think about how I would find that start if I didn't know.  The path I chose to go down (no pun intended) was to invert the tree, and create mappings from children-to-parents instead of the given parent-to-children structure.  This means that if the input had a parent `A -> ['B','C','C']` I would create three entires in my reverse tree for each child, all of which mapped to the parent value `A`.  \n",
    "\n",
    "Once that's done, I picked a random node ('dzzbkv' below) and started working my way up the parents, until their weren't any.  I have a feeling this might not be the best way to solve it, but it worked for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('day7.txt').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes = [re.findall('\\w+',node) for node in data]\n",
    "tree = {n[0]:n[2:] for n in nodes}\n",
    "reverse_tree = collections.defaultdict(list)\n",
    "for k,v in tree.items():\n",
    "    for node in v:\n",
    "        reverse_tree[node].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hlhomy'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def traverse(tree,start='dzzbvkv'):\n",
    "    \"\"\"pick a random start in the reversed tree, continue until nowhere to go\"\"\"\n",
    "    if tree[start]:\n",
    "        for n in tree[start]:\n",
    "            return traverse(tree,n)\n",
    "    return start\n",
    "traverse(reverse_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two\n",
    "Part two is tricky for me - it's still tree traversal, but you now have to incorporate the \"weights\" present at each node.  The way you incorporate them takes a bit of explaining so rather than regurgitate it here, just read the instructions on Advent of Code.\n",
    "\n",
    "In short, however, the idea is to make sure that the cumulative values of each child node of a parent are all equal, and therefore the parent is balanced.  One node is not balanced, and we need to find what it's value should be in order to make it balanced. Cumulative means that a node's value is the sum of it's children's values, plus it's own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571 66 1505\n",
      "22313 6292 16021\n",
      "111630 111573 57\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "data = open('day7.txt').read().strip().split('\\n')\n",
    "tree = [[list(re.match('(\\w+) \\((\\d+)\\)',x[0]).groups()),x[1:]]\n",
    "        for x in [line.split('->') for line in data]]\n",
    "\n",
    "new_tree = {}\n",
    "\n",
    "for node, neighbors in tree:\n",
    "    val = int(node.pop())\n",
    "    if neighbors:\n",
    "        neighbors = [n.strip() for n in neighbors[0].strip().split(',')]\n",
    "    new_tree[node[0]] = [val, neighbors]\n",
    "\n",
    "root = 'hlhomy'\n",
    "\n",
    "def traverse(tree,node=root):\n",
    "    __, children = tree[node]\n",
    "    if not children: \n",
    "        return __\n",
    "    for c in children:\n",
    "        val, children = tree[node]\n",
    "        # if found: return \n",
    "        tree[node][0] = traverse(tree,c) + val\n",
    "    __ , children = tree[node]\n",
    "    cvals = [tree[c][0] for c in children]\n",
    "    if len(set(cvals)) == 1:\n",
    "        return tree[node][0]\n",
    "    else:\n",
    "        unbalanced_value   = [c for c in cvals if cvals.count(c) == 1][0]\n",
    "        val_it_should_be   = [c for c in cvals if cvals.count(c) != 1][0]\n",
    "        unbalanced_node    = [c for c in children if tree[c][0] == unbalanced_value][0]\n",
    "        __, unbalanced_nodes_children = tree[unbalanced_node]\n",
    "        unbalanced_node_values = [tree[c][0] for c in unbalanced_nodes_children]\n",
    "        val_to_balance  = val_it_should_be - sum(unbalanced_node_values)\n",
    "        print(val_it_should_be,sum(unbalanced_node_values),val_to_balance)\n",
    "        return  val_to_balance\n",
    "\n",
    "print(traverse(new_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Eight\n",
    "Another mini-interpreter style problem.  This one is easy to parse, and the instructions are fairly limited.  I am almost certain we'll see a similar puzzle later with more involved parsing, and a greater set of operations to choose from.  Part 1 and 2 are both really solved by the same piece of code below, the only difference is that part 1 asks for the maximum register value *after* all instructions have run, while part 2 asks for the maximum value ever seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('day8.txt').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instructions = [re.match('(\\w+) (inc|dec) (-*\\d+) if (.*)',instr).groups() for instr in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "registers = {instr[0]:0 for instr in instructions}\n",
    "\n",
    "def process_expr(expr):\n",
    "    reg, test, val = expr.split(' ')\n",
    "    reg_val = registers[reg]\n",
    "    expr = (''.join(str(reg_val) + test  + str(val)))\n",
    "    return eval(expr)\n",
    "\n",
    "greatest_so_far = 0\n",
    "\n",
    "for reg,op,val,expr in instructions:\n",
    "    current_reg_val = registers[reg]\n",
    "    if process_expr(expr):\n",
    "        if op == 'inc':\n",
    "            registers[reg] = current_reg_val + int(val)\n",
    "        else:\n",
    "            registers[reg] = current_reg_val - int(val)\n",
    "    current_highest_val = max(registers.values())\n",
    "    if current_highest_val > greatest_so_far:\n",
    "        greatest_so_far = current_highest_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6696\n"
     ]
    }
   ],
   "source": [
    "print(greatest_so_far)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two\n",
    "Processing the instruction set and making the indicated modifications to the registers results in solving both problems at once - although I didn't know this until I read part two.  After going through all the instructions and changing the register values, part one is just the max of the register values, and the only change I needed to make for part two was adding the global `greatest_so_far` and the `if current_highest_val > greatest_so_far` test at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6061\n"
     ]
    }
   ],
   "source": [
    "print(max(registers.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day Nine\n",
    "I'm noticing a theme here... lot's of recursion and interpreters.  I tried to use recursion but got bit again by python's limit sinc the input text is much larger than 1,000, and had to re-write to some nested while loops, yet again.  I included the recursive version below anyhow, since I had already buit it on the tests cases before figuring out the input was too large for it to work.\n",
    "\n",
    "Part one and two are both solved below, there is only a slight modification for part two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('day9.txt').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tests pass'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_stream(s,val=1,garbage=False):\n",
    "    if not s: return 0\n",
    "    first, rest = s[0], s[1:]\n",
    "    if first == '{':\n",
    "        if not garbage:\n",
    "            return val + process_stream(rest,val + 1,garbage)\n",
    "        else:\n",
    "            return process_stream(rest,val,garbage)\n",
    "    elif first == '}':\n",
    "        if not garbage:\n",
    "            return process_stream(rest, val - 1,garbage)\n",
    "        else:\n",
    "            return process_stream(rest, val,garbage)\n",
    "    elif first == '!':\n",
    "        return process_stream(rest[1:],val,garbage)\n",
    "    elif first == '<':\n",
    "        return process_stream(rest,val,garbage=True)\n",
    "    elif first == '>':\n",
    "        return process_stream(rest,val,garbage=False)\n",
    "    else:\n",
    "        return process_stream(rest,val,garbage)\n",
    "        \n",
    "# print(process_stream(data))   #text > 1,000, over python's recursion limit :(\n",
    "\n",
    "def test():\n",
    "    assert(process_stream('{}') == 1)\n",
    "    assert(process_stream('{{{}}}') == 6)\n",
    "    assert(process_stream('{{},{}}') == 5)\n",
    "    assert(process_stream('{{{},{},{{}}}}') == 16)\n",
    "    assert(process_stream('{<a>,<a>,<a>,<a>}') == 1)\n",
    "    assert(process_stream('{{<ab>},{<ab>},{<ab>},{<ab>}}') == 9)\n",
    "    assert(process_stream('{{<!!>},{<!!>},{<!!>},{<!!>}}') == 9)\n",
    "    assert(process_stream('{{<a!>},{<a!>},{<a!>},{<ab>}}') == 3)\n",
    "    return \"tests pass\"\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests pass.\n"
     ]
    }
   ],
   "source": [
    "def process_stream_non_recursive(s,part=1):\n",
    "    val = 0\n",
    "    ix = 0\n",
    "    totals = []\n",
    "    garbage_count = 0\n",
    "    while ix < len(s):\n",
    "        if s[ix] == '{':\n",
    "            val += 1\n",
    "            totals.append(val)\n",
    "        elif s[ix] == '}':\n",
    "            val -= 1\n",
    "        elif s[ix] == '<':\n",
    "            ix += 1\n",
    "            while ix < len(s):\n",
    "                if s[ix] == '!':\n",
    "                    ix += 2\n",
    "                elif s[ix] == '>':\n",
    "                    break\n",
    "                else:\n",
    "                    garbage_count += 1\n",
    "                    ix += 1\n",
    "        ix += 1\n",
    "    if part == 1:\n",
    "        return sum(totals) #part one\n",
    "    else:\n",
    "        return garbage_count #part two\n",
    "\n",
    "def test_day9():\n",
    "    assert(process_stream_non_recursive('{}') == 1)\n",
    "    assert(process_stream_non_recursive('{{{}}}') == 6)\n",
    "    assert(process_stream_non_recursive('{{},{}}') == 5)\n",
    "    assert(process_stream_non_recursive('{{{},{},{{}}}}') == 16)\n",
    "    assert(process_stream_non_recursive('{<a>,<a>,<a>,<a>}') == 1)\n",
    "    assert(process_stream_non_recursive('{{<ab>},{<ab>},{<ab>},{<ab>}}') == 9)\n",
    "    assert(process_stream_non_recursive('{{<!!>},{<!!>},{<!!>},{<!!>}}') == 9)\n",
    "    assert(process_stream_non_recursive('{{<a!>},{<a!>},{<a!>},{<ab>}}') == 3)\n",
    "    print(\"tests pass.\")\n",
    "test_day9()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14212"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_stream_non_recursive(data,part=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6569"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_stream_non_recursive(data,part=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day Ten\n",
    "OK... this gets my vote for most obnoxious puzzle so far.  It was conceptually simple, but for me it was extremely tedious to code part 1.  I kept introducing OBO bugs and had a difficult time re-allocating the sorted array back in the main list.  Even though part 2 is considerably more involved, I found it straightforward once part 1 was out of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengths = [189,1,111,246,254,2,0,120,215,93,255,50,84,15,94,62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(data=list(range(0,256)),lengths=lengths):\n",
    "    pos = 0\n",
    "    skip = 0\n",
    "    for l in lengths:\n",
    "        end = pos + l\n",
    "        if end > len(data):\n",
    "            end = end % len(data)\n",
    "            right, left = data[pos:], data[0:end]\n",
    "            all_rev = list(reversed(right+left))\n",
    "            data[pos:] = all_rev[0:len(right)]\n",
    "            data[0:end] = all_rev[len(data[pos:]):]\n",
    "        else:\n",
    "            data = data[0:pos] + list(reversed(data[pos:end])) + data[end:]\n",
    "        pos = pos + skip + l\n",
    "        if pos > len(data):\n",
    "            pos = pos % len(data)\n",
    "        skip += 1\n",
    "    return data[0] * data[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38415"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9de8846431eef262be78f590e39a4848'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringify = str(lengths)[1:-1].replace(' ','')\n",
    "to_ascii = list(map(ord,stringify)) + [17, 31, 73, 47, 23]\n",
    "\n",
    "def process(data=list(range(0,256)),lengths=to_ascii):\n",
    "    pos = 0\n",
    "    skip = 0\n",
    "    for i in range(64):\n",
    "        for l in lengths:\n",
    "            end = pos + l\n",
    "            if end > len(data):\n",
    "                end = end % len(data)\n",
    "                right, left = data[pos:], data[0:end]\n",
    "                all_rev = list(reversed(right+left))\n",
    "                data[pos:] = all_rev[0:len(right)]\n",
    "                data[0:end] = all_rev[len(data[pos:]):]\n",
    "            else:\n",
    "                data = data[0:pos] + list(reversed(data[pos:end])) + data[end:]\n",
    "            pos = pos + skip + l\n",
    "            if pos > len(data):\n",
    "                pos = pos % len(data)\n",
    "            skip += 1\n",
    "    return data\n",
    "\n",
    "knotted = process()\n",
    "xord = [knotted[i:i+16] for i in range(0,len(knotted),16)]\n",
    "xord = [reduce(lambda x, y: x ^ y, segment) for segment in xord]\n",
    "to_hex = ''.join([hex(x)[2:] for x in xord])\n",
    "to_hex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 11\n",
    "When I read the description and provided test cases, I had no clue what was going on or how to map from directions to the provided distance test cases.  After some googling, and twitter mentions of redblob games (which I've used extensively in the past for it's ridiculously awesome graph search walk-throughs), I figured out how to move in a hex grid, and then the problem was straight-forward.\n",
    "\n",
    "That said - I still found the wording confusing.  I interprted the instructions as follows:\n",
    " - we are given a path taken by the \"child\"\n",
    " - find the end of the path, or the location the child ends up at\n",
    " - find the shortest number of steps to get to that location.\n",
    " \n",
    "Because of the \"shortest number of steps\" part, I had originally went down the path (no pun intended) of coding a Breadth First Search, after calculating the final location, to find the number of steps to that locaiton.\n",
    "\n",
    "It turns out all the problem really wanted was the distance from the start of that final location (it already is the shortest path).  You can see this by comparing the results of the BFS below vs the results of simply getting the distance of the end location from the start, or `(0,0,0)`.  They are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One & Two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('day11.txt').read().strip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759.0, 1501.0, [445, -759, 314])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [0,0,0]\n",
    "dirs = ['nw','n','ne','sw','s','se']\n",
    "hex_moves = [(-1,1,0),(0,1,-1),(1,0,-1),(-1,0,1),(0,-1,1),(1,-1,0)]\n",
    "coord_map = dict(zip(dirs,hex_moves))\n",
    "\n",
    "def hex_move(moves,start=(0,0,0)):\n",
    "    max_dist = 0\n",
    "    for m in moves:\n",
    "        start = vector_process(coord_map[m],start)\n",
    "        cur_dist = sum([abs(p) for p in start])/2\n",
    "        if cur_dist > max_dist:\n",
    "            max_dist = cur_dist\n",
    "    return cur_dist,max_dist,start # cur_dist = part 1, max_dist = part 2\n",
    "\n",
    "# assert(hex_move('ne,ne,ne'.split(',')) ==  3)\n",
    "# assert(hex_move('ne,ne,sw,sw'.split(',')) == 0) \n",
    "# assert(hex_move('ne,ne,s,s'.split(',')) == 2)\n",
    "# assert(hex_move('se,sw,se,sw,sw'.split(',')) ==  3)\n",
    "\n",
    "hex_move(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759\n"
     ]
    }
   ],
   "source": [
    "path, length = bfs((0,0,0),[445, -759, 314])\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 12\n",
    "The input is text in the form of `a <-> [b,c,d]`, representing a bi-directional graph as an adjacency list - so `a` can go to `b,c & d`, and the reverse is also true.  We are asked to find the number of \"programs contained in the ID of 0\", which is basically asking how many unique connections, either directly or indirectly, are there to 0.  \n",
    "\n",
    "For this I'll use a DFS starting at 0 and just return the length of everything that can be visited starting from that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('day12.txt').read().strip().split('\\n')\n",
    "data = [prog.split(' <-> ') for prog in data]\n",
    "data = {int(left):list(map(int,right.split(', '))) for left, right in data}\n",
    "\n",
    "def dfs(start,successors,cnt=0,visited=set()): # also cnt not incremented bc final stack frame is still 0 when returning up\n",
    "    if start not in visited:\n",
    "        visited.add(start)\n",
    "        for s in successors[start]:\n",
    "            dfs(s,successors,cnt + 1,visited) # had return here - bug, figure out why tomm\n",
    "    return len(visited)\n",
    "            \n",
    "dfs(0,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two\n",
    "Our twist for part two is that we want to know the total number of unique groups in the graph.  I'll just keep running dfs on every possible node and continually update a master set with all of the group sets generated by traversing each key.  I'm sure there is a better / more efficient way to do this by taking into account what's been seen and not revisiting those keys, but the dumb / brute force way worked fast enough for me, just letting the `seen` set filter out any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "def dfs(start,successors,cnt=0,visited=set()): # also cnt not incremented bc final stack frame is still 0 when returning up\n",
    "    if start not in visited:\n",
    "        visited.add(start)\n",
    "        for s in successors[start]:\n",
    "            dfs(s,successors,cnt + 1,visited) # had return here - bug, figure out why tomm\n",
    "    return visited\n",
    "\n",
    "seen = set()\n",
    "for k in data:\n",
    "    seen.add(frozenset(dfs(k,data)))\n",
    "    groups += 1\n",
    "print(len(seen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 13\n",
    "given a series of `layer:depth` integer pairs, we interpet the input as a firewall through which we need to move a packet, while avoiding \"scanners\".  A single line of input could be 3:5 meaning the third layer of the firewall has a depth of 5.  At each \"picosecond\" scanners move down one step until they hit the bottom depth, and then they move back up one step until they hit the top, oscillating up and down like an elevator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "Given all of the complete firewall, composed of many layer:depth units, we need to find the number of times a packet attempts to *enter* a layer which has a scanner present.  Packets move only along the top of each layer (depth 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1844"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('day13.txt').read().strip().split('\\n')\n",
    "# data = ['0: 3','1: 2','4: 4','6: 4']\n",
    "firewalls = [(int(d),'fwd',['s'] + list(range(int(r) - 1))) \n",
    "             for (d,r) in [line.split(': ') for line in data]]\n",
    "\n",
    "valid_depths = {d for d,dr,r in firewalls}\n",
    "missing_depths = set(range(max(valid_depths) + 1)) - valid_depths\n",
    "fill_in_missing = [(m,'fwd', ['e']) for m in sorted(missing_depths)]\n",
    "all_together = sorted(fill_in_missing + firewalls,key = lambda x: x[0])\n",
    "\n",
    "def elevator(coll,n,ix,dr):\n",
    "    while n > 0:\n",
    "        if dr == 'fwd':\n",
    "            ix += 1\n",
    "            if ix > len(coll) - 1:\n",
    "                ix -= 2\n",
    "                dr = 'bwd'\n",
    "        else:\n",
    "            ix -= 1\n",
    "            if ix < 0:\n",
    "                ix += 2\n",
    "                dr = 'fwd'\n",
    "        n -= 1\n",
    "    return (ix,dr)\n",
    "\n",
    "def move_scanner(depth_and_range):\n",
    "    d,dr,r = depth_and_range\n",
    "    if 'e' in r: return d,dr,r\n",
    "    loc = r.index('s')\n",
    "    r[loc] = '*'\n",
    "    new_loc,dr = elevator(r,1,loc,dr)\n",
    "    r[new_loc] = 's'\n",
    "    return d,dr,r\n",
    "\n",
    "penalty = []\n",
    "for i in range(len(all_together)):\n",
    "    d,dr,r = all_together[i]\n",
    "    if r[0] == 's':\n",
    "        penalty.append(d * len(r))\n",
    "    all_together = list(map(move_scanner,all_together))\n",
    "\n",
    "sum(penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'bwd', ['*', 's', '*']),\n",
       " (1, 'fwd', ['*', 's']),\n",
       " (2, 'fwd', ['e']),\n",
       " (3, 'fwd', ['e']),\n",
       " (4, 'fwd', ['*', 's', '*', '*'])]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_together[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two (and improved part one)\n",
    "I knew there was a simple way using modulo to do the elevator movements, but it just wasn't coming to me when I first tried the puzzle.  The next morning a coworker hinted at the modulo math required to tell if a scanner was at the top, and once I had that, the puzzle became infinitely simpler.  I'm including the original solution to part one (I could easily extend this for part two - at least I think I could) to keep myself honest, since I had a pretty big hint to arrive at the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1844\n",
      "3897604\n"
     ]
    }
   ],
   "source": [
    "data = [list(map(int,re.findall('\\d+',line)))\n",
    "        for line in open('day13.txt').read().strip().split('\\n')]\n",
    "\n",
    "fwalls = {layer:depth for layer,depth in data}\n",
    "fwalls_inc_missing = [(layer, fwalls.get(layer,0)) for layer in range(99)]\n",
    "\n",
    "def is_at_top(i,depth):\n",
    "    return i % ((depth - 1) * 2) == 0\n",
    "\n",
    "part1 = sum(l*d if is_at_top(l,d) else 0 for l,d in fwalls_inc_missing)\n",
    "print(part1)\n",
    "\n",
    "# part 2\n",
    "picosecond = 0\n",
    "safe = False\n",
    "while not safe:\n",
    "    safe = not(any(is_at_top(picosecond + layer,depth) \n",
    "                   for layer,depth in fwalls.items()))\n",
    "    if safe:\n",
    "        print(picosecond)\n",
    "        break\n",
    "    picosecond += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8222\n"
     ]
    }
   ],
   "source": [
    "salt = 'amgozmfv'\n",
    "\n",
    "def hex_to_bin(hex_digits):\n",
    "    return ''.join(map(lambda x: bin(int(x,16))[2:].zfill(4),hex_digits))\n",
    "\n",
    "def process(text,data=list(range(0,256))):\n",
    "    to_ascii = list(map(ord,text)) + [17, 31, 73, 47, 23]\n",
    "    pos = 0\n",
    "    skip = 0\n",
    "    for i in range(64):\n",
    "        for l in to_ascii:\n",
    "            end = pos + l\n",
    "            if end > len(data):\n",
    "                end = end % len(data)\n",
    "                right, left = data[pos:], data[0:end]\n",
    "                all_rev = list(reversed(right+left))\n",
    "                data[pos:] = all_rev[0:len(right)]\n",
    "                data[0:end] = all_rev[len(data[pos:]):]\n",
    "            else:\n",
    "                data = data[0:pos] + list(reversed(data[pos:end])) + data[end:]\n",
    "            pos = pos + skip + l\n",
    "            if pos > len(data):\n",
    "                pos = pos % len(data)\n",
    "            skip += 1\n",
    "    return data\n",
    "\n",
    "def knot_hash(text):\n",
    "    knotted = process(text)\n",
    "    segmented = [knotted[i:i+16] for i in range(0,len(knotted),16)]\n",
    "    xord = [reduce(lambda x, y: x ^ y, segment) for segment in segmented]\n",
    "    to_hex = ''.join([hex(x)[2:] for x in xord])\n",
    "    return to_hex\n",
    "\n",
    "grid = [hex_to_bin(knot_hash(salt + '-' + str(i))) for i in range(128)]\n",
    "print(len([sq for row in grid for sq in row if sq == '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0100011100111111101010011101100010101011001001000110001100011111000100001100001111100010000011011001101110000100100111010000',\n",
       " '1111110001111111101100111101011101010010001111100111001100100100101011000111100110111010000100100001001110101000011000111110',\n",
       " '0101101111101100000001001101101011110010010011010110001000010101111001110000111011110011011110011111000111110111001001001110',\n",
       " '0100000011101001111110000101100110110101111111110110100001110011111101111110101010101010101100000010100110010011110011111001',\n",
       " '11010100011001010110001010011110111010111100001101010111010101110010110110010000101100100100111001111101111000000110111011010111',\n",
       " '01010101110001111001111001001101110110101001000101100111101011100010011111011100101010001010011101110111111111000110100111010101',\n",
       " '01001010100101011001110010111010010110110011110011101100101010011000010111101111000111101110101010100100111000010011000000110000',\n",
       " '111010111101111110100100010101001001011100100100101011100100010011000010111101100100001100111010010010011001100010101011',\n",
       " '1000010001001011011111101001001000000110110110111100001101011010101001000111001010111011010111101111100010111011111011010101',\n",
       " '10010001010011011000011001101110100011110110111101110001001000110110111000011100101011011110001001100000010100011101110101010111',\n",
       " '0010110100100101010001010000110100000010011111100111011110001011101011101011011011011000010111100011010001000111110111011111',\n",
       " '100001011011111011001100111111011101011001110001010111011011011000011111101011000011101101100110110100001100100010000011',\n",
       " '10100110100101101011110101111110101011010110110111111011010101011101110101100000111010100010000110010010110101011011101100010111',\n",
       " '10100100011011001110111001111111100010010001111011000111010101010101110011111110110001111011000101111001001101100100100110001110',\n",
       " '1110000011111111110111010110111001100010100110110001001001001110001111000000000101011100100010111001011010001011000101110111',\n",
       " '100001010100111010111101101000011000100100111110111111010100101101000000100100010101000010100001101000010101000011111000',\n",
       " '000101110110101100010101110101110100110101011000101111111000100010010111000111010110100111000111101111010100100110010001',\n",
       " '01000100001000100111100101111010010111001101111010110011111000100001000001100010010101111100101001110100011000100011011001111001',\n",
       " '10111010100000100100011001000111101110001100110100100100100110111010111111011101011111111110001001110010001010000010',\n",
       " '1011100111000101110100010001011001111011111010101111011101110110101100100101100101010101101100011001110101011110100111110111',\n",
       " '1001000001011011111101100111100001110000111111011111111111111100111001000110011010000111011010100001000100011101110001011011',\n",
       " '01000101011010100010100010100111101011011111110001001110111010000001010000010101011101010101101010110010001001010011001011011100',\n",
       " '01101001001110000011110001000000010110100100100010111111110000010010011110011100110000110010101110010011010011000001',\n",
       " '000110011010111111111100110111110011010010000010010001110110101000010111111010111110000001011111001000111011011100110111',\n",
       " '01010010111100000011000110100100101010110100001011101000001010001011000011011100010000000101010111100110100010100010001100101000',\n",
       " '1110111110110010111110011001110101010011100010111111110101011010101001011001000010100011011010010010001001111101011011100000',\n",
       " '001111001011100000100001111101001001010010100010101001101000011101110010000010001100100010111011110111111110110100011011',\n",
       " '11111001110100100111111010000110011011101100100011110110001101111100101000110101011001100010100111010010101000101100010010110000',\n",
       " '0101001110101001111001001100011010101010110111000111010001011001110011001111110011011100000110110110011011110011000010000111',\n",
       " '10111001010111001110001110011001110100101001011000111001010100111000111111100101011100110110000010011111000100100001001001010111',\n",
       " '0100000010100000010000011111111010100011100110100111100100101000111011000110110101001100000110100110011100011001101110100110',\n",
       " '11000011100110011010010010100000000101100010101110011110111100001001001010001010100110101110101101011010010011110011111101001110',\n",
       " '111010100001110010010011001011110111111001000110010110101001001010000011011000000111110100011110000101011111011100110110',\n",
       " '001000100110111010101011001101100100001001100010010101111111111110010111001101100011110110011101110000000110100111110001',\n",
       " '101101001001111010011000101011000100001010000111010111110101100001101000111100000100011101000111000101101110001111010111',\n",
       " '111001111111001011000111101000101101011100011101100011011000001111101100011110110111001011001100011100010001110101001010',\n",
       " '011100011000011110001110110010010111110001110100101101101101001000011010101001111011001111011001010000100110100000010011',\n",
       " '10001101100100011110111010100110001000000100110111001110000100001000101011110111010101011010011010010101010010001111',\n",
       " '10110100001100010110110001000101111100100010000010101001101010000101101011101111101100101010001100010111110010010010111000101011',\n",
       " '1001111101001100101011001000111010001001011100101000010100000100101000100111010011001111100111001101101110100101110001111100',\n",
       " '1001101001010111100000111101101111011001010010100010001111000110001110001011101000011101111011000001000111011011111110111011',\n",
       " '1000010010100011001100100111100101101100100101001011001001110011100010010110110011100100010011100010011011110011101011110110',\n",
       " '001110111011010000010100110001001110001100011000000111011101110110010100011111000100001111101110110100111111000111011110',\n",
       " '0100001111110100110100101101000000001011100110110001100111101111001011011111010100110010101111110000011001010110111010001101',\n",
       " '101011011000010110100101100000110011110111110101111011100011010110111111101000010110100100001010111101110111110111011001',\n",
       " '11001111110110010111000000010000011101100101000101000011011001111000110001100011011101111101100011111101101000010101000000111001',\n",
       " '001001001110001101101111101001011000100111101001010111001110101100101100111111101111011111011101101110000110011110011011',\n",
       " '111110000101000000111111000101000100101101100100110111111101111101010000101111100011100011011101100011110110111110001011',\n",
       " '1101010011100000000100111100100101111000100111110100000000111011101001011010110000010110101010001100111000000111001101011010',\n",
       " '1101111001000000111101011110000010010111010011000010010101101011101110000110111010000100110010001110100000111001010011111011',\n",
       " '0001101101110100100010000011110111110010100100011110001011001000011011101010100111111000110010100010010101110110110001111101',\n",
       " '01100100110011101110000101010110001010010111000010000011110010111010110000110010001010011101110100101110001110100001001101100001',\n",
       " '01011001101111001011011001000010101111110111111110100100011100101111101101011110111111010101010111111101101111100001101001010011',\n",
       " '00011000110011010011000110000001011100100001100110010101010110000011001001101100010101101111011000111111100001000010010010100010',\n",
       " '11001110111101000100100111001000000111010010010001110111011110100001001000110100100011111111010011101011111011101010',\n",
       " '011000001011111010001111001110001000010111110101010101110111010001110010110101010111000011010100000110101010100011101101',\n",
       " '1110001001110001110001010110101100111001100000100011001111110110000010111100001001000111000010010110000110111110000001011111',\n",
       " '10111111110000100011100000100111101111110111001111001101100100101100000100011110100111110010011100110111010010010110',\n",
       " '01110101100000110100000111110000111010111111000001101111100010100101101101110011010000001111110101001101001010000101110100010100',\n",
       " '0011101000100010100100000100101011110011100010011100111101000110100101011100001101101001010011000111001011010101011101101010',\n",
       " '0001010000100111001110110110000001101011010111110010000110101100001110011011011110111010010110100101001001110111011010011100',\n",
       " '101000010001011001001000101111101111000110001011100101100110110000110111011011001110110001101000111111100100010010000110',\n",
       " '0011111000101001001110000001000100010111001010100011111100111011001011001010000000011100101111101010000101001110010101011010',\n",
       " '1000101100110110011000001010101011101101111100001101010110100010000111011001000100110111101111110110111101000101000110111010',\n",
       " '11101000010010111101101101011110010110111100000110100011110011100010001011110101111001010011011110011011101011111010010101000101',\n",
       " '1101000101100001111100001111000111110111111011111001000010110000010110110100011011100001111101001001111010110100010010101111',\n",
       " '11000000000101110100001100101010110111101111001111011101111110111011001011110110110011100111001011010000110110111011100011111110',\n",
       " '0100001011111111001000111001110001101011011110000011001001010100111101111000101010001011000001111001001110101101001111101110',\n",
       " '10010101001101101111100101010001000111000111011111111001100010101101001011100110001010011011110011011000000110011110111010011101',\n",
       " '01100111001101001101101001000100001100110111100001000110010011110001010110000110100110010111011001110100111001000010001101000000',\n",
       " '1001010010101011001100101101100000101101011111100000111001111010000111011011100000010111011101001100111011010011110100010111',\n",
       " '0110110111101110010000001010010101000010001111000011111000011001111101101100110110011110110010001110011010101011001100111110',\n",
       " '00101110111011100110110110101000010111111011101011011011010010011011001100010101101101100011001011001101101010011111101011001110',\n",
       " '00110001111101010010101010100100010010010110000101101111010001000010000110001011100010011110000100110010101111111000101010001100',\n",
       " '0001011001100100110001100100111101000101011011101011000001101000011000100011110001000001110010111011011011001011000101111101',\n",
       " '1111000010001100100010111111010010011101011111010101011001010101100010001000000110101100110000011111100110001001011010110110',\n",
       " '1111101011110001110110010101100010111111110001110111000110000111011101101100000010001111101011001110111110101001110101000011',\n",
       " '10101001111000000100011110001001110110010101000010111111001100100011000111110011110100000010010110101001001111101011100010011011',\n",
       " '10010111100000111001000101100001010010000110000101100000000100000011010000101000000110110011100000110111111111100001100101010010',\n",
       " '01010010110111011111110011110100100101010001111010001010001001110110110110101110100100001100000001111110100001011111101100110010',\n",
       " '10011110011011100110100111000010110101111001101110101100001100111000010110001011110010110011111111000111000111010010001010001010',\n",
       " '01110101100110001011000000100000000110100111001101110000011010101110111111000111110010001111011101100100110100110100',\n",
       " '110111100001011101010010001110111000001111111101011100110010111100010011001010111111111000101110101100110100001011101100',\n",
       " '11100011011101001111110110011110111001010001111101001111001110111010101000100110100000101111111111000111110000000110010111101001',\n",
       " '000101000111101001111010110100110110011010110111011010010010110001011101111001000110110100001100011001010100000001000000',\n",
       " '1000110111011001010101111011111011111000001111101010110101110110101101000011101001010110000101111101111101001000111111100110',\n",
       " '10001010001001000110011001111010110101110101011011100111111110010100000111101010100001000001100100111001001101101001101010001110',\n",
       " '10000000110111111010010001111111100100110110010010110000101011001001101110000011111010101001101001101100110001000011100110010110',\n",
       " '11010111101001001001011011111001011111100110101101010111110011100110110000101001101111000011110111101000011010010101101010001111',\n",
       " '01001011011000111010000010010101100000100010000001111111101000100111010001101111001010110010011110000111100000100001000101100001',\n",
       " '1010000110111010100000011110000010001000001010110001101000010110100001010011100110110011100111011101110100101100000011100011',\n",
       " '10001011001101011011001001001111101110100100001111000011010101001111011010000100101010111010011010110010101111101100101010010100',\n",
       " '101101100011101100111010000000010110001111100001000001111001100110001011011110000101001111010110011110101100000111101111',\n",
       " '0010010011100010000100101110101100110011100101011111000100010101010011010011001000011100001101110010101010100110110110101000',\n",
       " '001011000101000111100101000100001011000110001100110000000110000111001010101011110010010010110010010111111011110010001100',\n",
       " '01010111110010000011111000010010010011100101100110011100001001011111101110101011010110001110011001110010000111111011110100100011',\n",
       " '11001110010110000010011000101101101000000111001001100101000011111100011100101100100111101101110111010100101001100000',\n",
       " '001011111111100110010111010001011001110111111010100001001001000111000101001001001110010110010110111000000101110010100111',\n",
       " '10010001110100111010011101111111000111101110010110011010001010111011100100110010101011110111111101001011001101111101011100100000',\n",
       " '0110010000101001101000010001110110111101111100101000001101110001011001110100100000011101011001111001111010000010101011010010',\n",
       " '1110111011000000111110000010111000010111000100100110011101011010111110011011101110100010101011011101100010100011111001000011',\n",
       " '10111011111010101100000001001100011111110100101000101111001011001000100101110110111111101010011110001101011010110010100010000011',\n",
       " '00111001101100110010110010101100001010000001100110000110111000001111111111101110010010001101111011100101001100001011010010111011',\n",
       " '011110001110100110111001101110100100101001011111111000101100101010110010101010000111100010010011000110101111000000011110',\n",
       " '00111110011111011100010001010000001100001100101000011011001011111010011011100010010101111111101111100110010000111010110011111000',\n",
       " '0101010001111100101011000110000010001000101011000011011010110100111101010110100001011100011111101110110010111000100000111000',\n",
       " '11111111000000110001011001100100011011101101000111111010100011100100100001111101011101111001001001110101110101000111',\n",
       " '1101011011000000101010100010011111001010011010010100010011001011001011100011011011001001001000011111010101101111110110000101',\n",
       " '0010001011011100111001011111011010110000100100101101100001000110001100011100001111010101001100010001010010101110110010010111',\n",
       " '00010110001000101011010110100110011100001011110100011011101010111011111110101110010000011001111000110110011110101010011001111110',\n",
       " '1110001111011110100011111000010011001010010100111001110110000000100110110000011101011111101000101100011101000100000110011101',\n",
       " '1110011111011100111011110001110110100001101001100110011000011000111101110010100111101111100000110001111111101101110100100000',\n",
       " '0100001001111111001110011111000110100000100000000010111110100011001111101010100110100100110110110100001001001100011111011110',\n",
       " '11011010101000001000100001111111110001011000010010110101001100001111101010001101001011011011111000110010111110101011001111010110',\n",
       " '1110011111011101110011101110010010111010011100110111100011111011001101001001110101010101100110010011000100100100101100010010',\n",
       " '0101110100101101100101000001100000011111110010101001011100101110111100110011110010101000000010001000011101110100001010110101',\n",
       " '0010011111100101101100010101111001000101100111000101000111000101100110101000111001111010100100111010001110001001101001110001',\n",
       " '10011101100100001010001100110010101100011011110110010101010000101110110001000100000001011111111101011111110001100100',\n",
       " '0101100100010101001011110000001000000111010101111011110011000110010101011001010001101110110110100111110110111111000010011010',\n",
       " '00111010010110110001011111000100111100111001011101111011011011110000010100000010001000100000010101011001011100010110',\n",
       " '011011100100011011111110000111111111001101001011110111001110111111101000001011101101111101011111000110000011011100111010',\n",
       " '0001000010110001111110101011000010000110100100001010011010000010010100000111001101010010110111001111010011001000000001001000',\n",
       " '01100010011001110001110001111001010010111110101011100111101011011001010110011010000101000111101101101100001111110101010111101101',\n",
       " '00011100011110111001010111110101100101110111011001010010101010001000111010001111111101101000100011000011110011101010001011001100',\n",
       " '11111111111110110110011001100010101110110100110011101100100011000111011100010000101110011111000001001000110111101000011110101000',\n",
       " '01101110111000111011101110010010011111011001100100010100000111000011111100101111110011010110000010000101101100110100101010001001',\n",
       " '011111111101001111011001101000100010100111001110011110000011010110111101010010101100110001100001011011111001011100100000',\n",
       " '11111111010001100011100001101110100110000001101100000010010001001101111010010111011111101011111010000001010000010101']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-436-0293627b2b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#maybe flatten seen first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneighbors_URDL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-436-0293627b2b8b>\u001b[0m in \u001b[0;36mdfs\u001b[0;34m(grid, start, successors, visited)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                         and grid[i][j] == '1'):\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuccessors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[0;32m<ipython-input-436-0293627b2b8b>\u001b[0m in \u001b[0;36mdfs\u001b[0;34m(grid, start, successors, visited)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                         and grid[i][j] == '1'):\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuccessors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvisited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvisited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "def neighbors_URDL(coord,is_valid_coord = lambda x: x):\n",
    "    moves = [vector_process(coord,move) for move in [(-1,0),(0,1),(1,0),(0,-1)]]\n",
    "    valid_moves = [tuple(m) for m in moves if is_valid_coord(m)]\n",
    "    return valid_moves\n",
    "    \n",
    "def is_valid_coord(xy):\n",
    "    x,y = xy\n",
    "    return(0 <= x < 128) and (0 <= y < 128) \n",
    "\n",
    "def get_clusters(grid,loc):\n",
    "    return dfs(grid,loc,successors=lambda x: neighbors_URDL(x,is_valid_coord),visited=set())\n",
    "\n",
    "seen = set()\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        if (i,j) not in set.union(*seen): \n",
    "            seen.add(frozenset(dfs(grid,(i,j))))\n",
    "print(len(seen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "A = 699\n",
    "B = 124\n",
    "\n",
    "factor_a = 16807\n",
    "factor_b = 48271\n",
    "divisor = 2147483647\n",
    "\n",
    "def generator(val,factor,multiple):\n",
    "    while True:\n",
    "        val = val * factor % divisor\n",
    "        if val % multiple == 0:\n",
    "            yield val\n",
    "        \n",
    "ga = generator(A,factor_a,4)\n",
    "gb = generator(B,factor_b,8)\n",
    "\n",
    "total = 0\n",
    "for i in range(5000000):\n",
    "    a = bin(next(ga))\n",
    "    b = bin(next(gb))\n",
    "    if a[-16:] == b[-16:]:\n",
    "        total += 1\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('aoc16.txt').read().strip().split(',')\n",
    "programs = 'abcdefghijklmnop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x15/10', 'ph/c', 'x3/14', 's1', 'pf/b', 'x12/0', 'ph/m', 'x14/11', 's4', 'x6/13']\n"
     ]
    }
   ],
   "source": [
    "print(data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bkgcdefiholnpmja'"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spin(sx,programs):\n",
    "    x = int(sx[1:])\n",
    "    return programs[-x:] + programs[0:len(programs)-x]\n",
    "\n",
    "def exchange(xab,programs):\n",
    "    ix_a,ix_b = list(map(int,re.findall('\\d+',xab)))\n",
    "    programs[ix_a], programs[ix_b] = programs[ix_b], programs[ix_a]\n",
    "    return programs\n",
    "\n",
    "def partner(pab,programs):\n",
    "    a,b = pab[1:].split('/')\n",
    "    ix_a, ix_b = programs.index(a), programs.index(b)\n",
    "    programs[ix_a], programs[ix_b] = programs[ix_b], programs[ix_a]\n",
    "    return programs\n",
    "\n",
    "def process(moves,programs):\n",
    "    programs = list(programs)\n",
    "    for m in moves:\n",
    "        dance_move = m[0]\n",
    "        if dance_move == 'x':\n",
    "            programs = exchange(m,programs)\n",
    "        elif dance_move == 's':\n",
    "            programs = spin(m,programs)\n",
    "        elif dance_move == 'p':\n",
    "            programs = partner(m,programs)\n",
    "        else:\n",
    "            raise ValueError('no dance type found that matches!')\n",
    "    return ''.join(programs)\n",
    "\n",
    "process(data,programs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two\n",
    "Now we do the same thing, but 1 billion times.  Brute force isn't going to work here, so I am going to check to see if there is a discernible pattern in the dance by looking for cycles.  It turns out, for my input, after 60 iterations we are back where we started.  To be more certain I should test that this pattern holds for a few more cycles of 60, but in the interest of time I just assumed it would and proceeded - thinking I'd come back and try again if it didn't hold. \n",
    "\n",
    "Since every 60 dances, we are back at the start, We can just run the dance cycles starting at the beginning, for the remainder of 1 billion divided by 60, or 40 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "abcdefghijklmnop\n"
     ]
    }
   ],
   "source": [
    "programs = 'abcdefghijklmnop'\n",
    "seen = set()\n",
    "while programs not in seen:\n",
    "    seen.add(programs)\n",
    "    programs = process(data,programs)\n",
    "\n",
    "print(len(seen))\n",
    "print(programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knmdfoijcbpghlea'"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programs = 'abcdefghijklmnop'\n",
    "for x in range (1000000000%60):\n",
    "    programs = process(data,programs)\n",
    "programs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
